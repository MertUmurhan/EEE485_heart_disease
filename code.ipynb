{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["drive.mount(\"/content/gdrive/\")\n","!ls \"/content/gdrive/MyDrive/ee/\""],"metadata":{"id":"kF215b7OEsPI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","!ls \"/content/gdrive/MyDrive/EEE485/\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3rLndbEciI52","executionInfo":{"status":"ok","timestamp":1649528150519,"user_tz":-180,"elapsed":2208,"user":{"displayName":"Senih","userId":"00479760256499893066"}},"outputId":"dfd079ca-69a1-400b-c2b7-7a9292b6f9dd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n","ls: cannot access '/content/gdrive/MyDrive/EEE485/': No such file or directory\n"]}]},{"cell_type":"code","execution_count":1,"metadata":{"id":"_OzkPGUJSP0h","executionInfo":{"status":"ok","timestamp":1652342678247,"user_tz":-180,"elapsed":255,"user":{"displayName":"Mert Umurhan","userId":"15347790837865452233"}}},"outputs":[],"source":["import os\n","import math\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from random import randrange\n","from random import seed\n","from google.colab import drive"]},{"cell_type":"code","source":["def summary(data):\n","    # Looking first few rows of the data\n","    print(data.head())\n","\n","    # Looking for the data and missing values\n","    print(data.info())\n","\n","    # Looking the summary of the data\n","    print(data.describe())\n","    return None\n","\n","def cleaner(data):\n","    data = data.iloc[:, 1:] # Deselecting first column\n","    data.index.name = 'id' # Renaming index of the first column\n","    data.dropna(inplace = True) # Removing null values\n","    data['age'] = data['age']/365 # Transforming age into years\n","    data = data[(data.height>=140) & (data.height<=220)] # Filtering height to be at least 140cm or at most 220cm\n","    data = data[(data.ap_hi>=50) & (data.ap_hi<=1000)] # Filtering systolic pressure to be at least 50mmHg or at most 300mmHg\n","    data = data[(data.ap_lo>=20) & (data.ap_lo<=1000)] # Filtering diastolic pressure to be at least 20mmHg or at most 300mmHg\n","    data = data[data.weight>=40] # Filtering weight to be at least 40\n","    \n","    # Power transform the data\n","\n","    return data\n","\n","def visualizer(data):\n","\n","    # Scatter plot with cardio against height\n","    # plt.scatter(data['cardio'], data['height'], c=data['gender'])\n","    \n","    # Adding Title to the Plot\n","    # plt.title(\"Scatter Plot\")\n","    \n","    # Setting the X and Y labels\n","    # plt.xlabel('cardio')\n","    # plt.ylabel('height')\n","    \n","    # plt.colorbar()\n","\n","    # plt.show()\n","    \n","    pd.crosstab(data['cholesterol'], data['cardio']).plot.bar(stacked=False)\n","    plt.figure(1)\n","    pd.crosstab(data['active'], data['cardio']).plot.bar(stacked=False)\n","    plt.figure(2)\n","    pd.crosstab(data['gluc'], data['cardio']).plot.bar(stacked=False)\n","    plt.figure(3)\n","    pd.crosstab(data['smoke'], data['cardio']).plot.bar(stacked=False)\n","    plt.figure(4)\n","    pd.crosstab(data['alco'], data['cardio']).plot.bar(stacked=False)\n","    plt.figure(5)\n","    pd.crosstab(data['gender'], data['cardio']).plot.bar(stacked=False)\n","    plt.show()\n","\n","    return None\n","\n","def splitter(seed, data, train = 0.60, val= 0.20):\n","    if not isinstance(data, (np.ndarray)):\n","      data = data.to_numpy()\n","\n","    np.random.seed(seed)\n","    np.random.shuffle(data)\n","\n","    indexOfTrain = int(len(data)*train)\n","    indexOfVal = int(len(data)*(train+val))\n","\n","    train = data[:indexOfTrain]\n","    val = data[indexOfTrain:indexOfVal]\n","    test = data[indexOfVal:]\n","\n","    return train, val, test\n","\n","def normalization(datas): # takes numpy array\n","  stds = np.std(datas, axis=0)\n","  means = np.mean(datas,axis=0)\n","  for i in range(11):\n","    datas[:,i] = (datas[:,i] - means[i])/stds[i]\n","  return datas\n","# pd.set_option('display.max_rows', None, 'display.max_columns', None) # Viewing the output without truncation\n","data = pd.read_csv(\"cardio_train.csv\", header= 0, sep= \";\")\n","\n","def label_design(data): # takes numpy array\n","  labels = data[:, 11]\n","  datas = np.delete(data, 11, 1)\n","  return datas, labels"],"metadata":{"id":"Yp_DLGC6aCIR","executionInfo":{"status":"ok","timestamp":1652342679727,"user_tz":-180,"elapsed":268,"user":{"displayName":"Mert Umurhan","userId":"15347790837865452233"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Looking at the data\n","summary(data)\n","\n","# Cleaning the data\n","data = cleaner(data)\n","\n","# Looking once again\n","summary(data) # Now data looks better\n","\n","# Visualisation of the data\n","visualizer(data)\n","\n","# Splitting data\n","train, val, test= splitter(1, data)\n","# print(np.shape(train))\n","# print(np.shape(test))"],"metadata":{"id":"IZokCPKYNuer"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Logistic Regression"],"metadata":{"id":"0ED0Gsu4mCvj"}},{"cell_type":"code","source":["class lr:\n","  def __init__(self, learning_rate=1e-4, threshold = 0.5):\n","    self.learning_rate = learning_rate\n","    self.threshold = threshold\n","    self.w = None\n","  def initialize_weight(self,dim):\n","    self.w = np.zeros((dim,))\n","    return None\n","\n","  def sigmoid(self, x): \n","    a = np.exp(np.dot(x,self.w))\n","    return a/(1 + a)\n","\n","  def fit(self, x_train, y_train):\n","    dim = x_train.shape[0]\n","    i = 0\n","    pre_loss = 0\n","    loss = 1\n","    while abs(loss - pre_loss) > 0.01:\n","\n","      i += 1\n","      pre_loss = loss\n","      gradient = (np.dot(x_train.T, (self.sigmoid(x_train) - y_train)))/dim\n","      self.w = self.w - gradient*self.learning_rate\n","      acc = self.accuracy(y_train, self.sigmoid(x_train) > 0.5 )\n","      y_prob = self.sigmoid(x_train)\n","      loss = np.sum(-y_train*np.log(y_prob)-(1-y_train)*np.log(y_prob))\n","      #print(np.linalg.norm(gradient))\n","      print(i)\n","      print(\"Loss: \",loss)\n","      print(\"Accuracy: \",acc)\n","\n","  def test(self, x_test):\n","    return self.sigmoid(x_test) > self.threshold \n","\n","  def accuracy(self,y_test,y_est):\n","    return np.sum(y_est == y_test)/np.size(y_test)"],"metadata":{"id":"xCzBUeIwOqO-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_train, y_train = label_design(normalization(train))\n","x_val, y_val = label_design(normalization(val))\n","x_test, y_test = label_design(normalization(test))\n","model = lr(1,0.5)\n","model.initialize_weight(11)\n","model.fit(x_train, y_train)\n"],"metadata":{"id":"t67Ty0Mks0FC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["accs = []\n","for i in range(100):\n","  model.threshold = 0.4 + i/500\n","  y_est = model.test(x_val)\n","  accs.append(np.sum(y_est == y_val)/np.size(y_val))\n","plt.plot(np.linspace(0.4, 0.6, num=100),accs)\n","plt.ylabel('accuracy')\n","plt.xlabel('threshold')\n","plt.show()\n","\n","index_max = np.argmax(accs, axis=0)\n","print(\"Threshold: \", 0.4 + index_max/500)\n","print(\"Accuracy: \",np.max(accs))"],"metadata":{"id":"ZSDkULWBFPSa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.threshold = 0.5\n","y_result = model.test(x_test)\n","print(\"Test Accuracy: \",model.threshold,model.accuracy(y_test,y_result))\n","model.threshold = 0.45\n","y_result = model.test(x_test)\n","print(\"Test Accuracy: \",model.threshold,model.accuracy(y_test,y_result))"],"metadata":{"id":"3Y2vbmNgihjA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["SVM - Linear"],"metadata":{"id":"GFapzIZOfNJW"}},{"cell_type":"code","source":["class svm:\n","    # Linear Soft Margin SVM\n","    def __init__(self):\n","        self.w = None\n","\n","    def fit(self, x_train, y_train, x_val, y_val, learning_rate=1e-5, epoch=30, regularization_=0.1):\n","        val_hist = []\n","        w_list = []\n","        y_ = (y_train-0.5)*2\n","        self.w = np.zeros(x_train.shape[1])\n","\n","        for i in range(epoch):\n","            print(i,\"Train Accuracy: \",self.accuracy(y_train,self.test(x_train)))\n","            for i in range(len(x_train)):\n","                if (y_[i] * (np.dot(x_train[i], self.w.T))) >= 1:\n","                    self.w = self.w - learning_rate * (2 * regularization_ * self.w)\n","                else:\n","                    self.w = self.w - learning_rate * (2 * regularization_ * self.w - np.dot(x_train[i], y_[i]))\n","            val_hist.append(self.accuracy(y_val,self.test(x_val)))\n","            w_list.append(self.w)\n","        return val_hist\n","\n","    def test(self, x_test):\n","      test_result = []\n","      for i in range(len(x_test)):\n","        test_result.append(np.dot(x_test[i], self.w))\n","      \n","      return np.sign(np.array(test_result))\n","\n","    def accuracy(self,y_test,y_est):\n","      return 100*np.sum(y_est == y_test)/np.size(y_test)"],"metadata":{"id":"My_UH-jVfObu","executionInfo":{"status":"ok","timestamp":1652345665548,"user_tz":-180,"elapsed":298,"user":{"displayName":"Mert Umurhan","userId":"15347790837865452233"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["x_train, y_train = label_design(normalization(train))\n","x_val, y_val = label_design(normalization(val))\n","x_test, y_test = label_design(normalization(test))\n","model = svm()\n","hist = model.fit(x_train, (y_train-0.5)*2, x_val, (y_val-0.5)*2, learning_rate=1e-6, epoch=200, regularization_=0.01)"],"metadata":{"id":"x7r-rdwSc85u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(np.arange(1,len(hist)+1),hist)\n","plt.show()\n","\n","epoch_max = np.argmax(hist, axis=0)\n","print(\"epoch: \", epoch_max)\n","print(\"Accuracy: \",np.max(hist))"],"metadata":{"id":"W-XChsncb1nO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_fin = svm()\n","hist_fin = model_fin.fit(x_train, (y_train-0.5)*2, x_test, (y_test-0.5)*2, learning_rate=1e-6, epoch=epoch_max, regularization_=0.01)\n","print(\"Test Accuracy: \",hist_fin[-1])"],"metadata":{"id":"dYYEVdTEfFZX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["SVM with sklearn"],"metadata":{"id":"Z6jnjQGAJxsu"}},{"cell_type":"code","source":["from sklearn import linear_model, svm, discriminant_analysis, metrics\n","from scipy import optimize\n","x_train, y_train = label_design(normalization(train))\n","x_val, y_val = label_design(normalization(val))\n","x_test, y_test = label_design(normalization(test))\n","\n","model = svm.SVC(kernel='rbf', C=10, gamma=1.5, shrinking=False)\n","model.fit(x_train, y_train);"],"metadata":{"id":"k64o53GOIHjt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.kernel_approximation import RBFSampler\n","from sklearn.linear_model import SGDClassifier\n","x_train, y_train = label_design(normalization(train))\n","x_val, y_val = label_design(normalization(val))\n","x_test, y_test = label_design(normalization(test))\n","for i in np.arange(100, 1001, 100):\n","  rbf_feature = RBFSampler(gamma=0.1, n_components=i, random_state=1)\n","  X_features = rbf_feature.fit_transform(x_train)\n","  clf = SGDClassifier(max_iter=1000)\n","  clf.fit(X_features, y_train)\n","  xt_features = rbf_feature.fit_transform(x_val)\n","  y_est = clf.predict(xt_features)\n","  accuracy = np.sum(y_est == y_val)/np.size(y_val)\n","  print(i,\" :\",accuracy)\n","#print(accuracy)"],"metadata":{"id":"eW-kWNgUPeoq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["xt_features = rbf_feature.fit_transform(x_test)\n","y_est = clf.predict(xt_features)\n","accuracy = np.sum(y_est == y_test)/np.size(y_test)\n","print(accuracy)"],"metadata":{"id":"cg-_rpZjJ40P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Decision Tree"],"metadata":{"id":"97vDL0P2N0Rr"}},{"cell_type":"code","source":["def gini_index(groups, classes):\n","\n","    numOfInstances = float(sum([len(group) for group in groups]))\n","    gini = 0.0\n","\n","    for group in groups:\n","        size = float(len(group))\n","        \n","        if size == 0:\n","            continue\n","        score = 0.0\n","\n","        for class_value in classes:\n","            p = [row[-1] for row in group].count(class_value) / size\n","            score += p**2\n","        \n","        gini += (1.0 - score) * (size/numOfInstances)\n","    \n","    return gini\n","\n","def feature_split(index, threshold, dataset):\n","    below, above = list(), list()\n","\n","    for row in dataset:\n","        if row[index] < threshold:\n","            below.append(row)\n","        else:\n","            above.append(row) # Greater than or equal to values\n","\n","    return below, above\n","\n","def find_split(dataset):\n","\tclasses = list(set(row[-1] for row in dataset))\n","    \n","\tbest_col, best_value, best_score, best_groups = 999999, 999999, 999999, None\n","\tfor col in range(len(dataset[0])-1):\n","\t\tfor row in dataset:\n","\t\t\tgroups = feature_split(col, row[col], dataset)\n","\t\t\tgini = gini_index(groups, classes)\n","\n","\t\t\tif gini < best_score:\n","\t\t\t\tbest_col, best_value, best_score, best_groups = col, row[col], gini, groups\n","\n","\treturn {'index':best_col, 'value':best_value, 'groups':best_groups}\n","\n","def terminal(subset):\n","\tresult = [row[-1] for row in subset]\n","\treturn max(set(result), key = result.count)\n","\n","def child_split(node, maxDepth, minSize, depth):\n","    below, above = node['groups']\n","    del(node['groups'])\n","\n","    if not below or not above:\n","        node['below'] = node['above'] = terminal(above + below)\n","        return\n","    if depth >= maxDepth:\n","        node['below'], node['above'] = terminal(below), terminal(above)\n","\n","    if len(below) <= minSize:\n","        node['below'] = terminal(below)\n","    else:\n","        node['below'] = find_split(below)\n","        child_split(node['below'], maxDepth, minSize, depth+1)\n","    \n","    if len(above) <= minSize:\n","        node['above'] = terminal(above)\n","    else:\n","        node['above'] = find_split(above)\n","        child_split(node['above'], maxDepth, minSize, depth+1)\n","\n","def decision_Tree(train, maxDepth, minSize): # Random Forest might be implemented in the final because of bias and overfitting possibilities\n","    rootNode = find_split(train)\n","    child_split(rootNode, maxDepth, minSize, 1)\n","    return rootNode\n","\n","def treeShow(node, depth = 0):\n","    if isinstance(node, dict):\n","        print('%s[X%d < %.3f]' % ((depth*' ', (node['index']+1), node['value'])))\n","        treeShow(node['below'], depth+1)\n","        treeShow(node['above'], depth+1)\n","    else:\n","        print('%s[%s]' % ((depth*' ', node)))\n","\n","def treePredict(node, row):\n","    if row[node['index']] < node['value']:\n","        if isinstance(node['below'], dict):\n","            return treePredict(node['below'], row)\n","        else:\n","            return node['below']\n","    else:\n","        if isinstance(node['above'], dict):\n","            return treePredict(node['above'], row)\n","        else:\n","            return node['above']\n","\n","def main_tree(train, test, max_depth, min_size):\n","\ttree = decision_Tree(train, max_depth, min_size)\n","\tpredictions = list()\n","\tfor row in test:\n","\t\tprediction = treePredict(tree, row)\n","\t\tpredictions.append(prediction)\n","\treturn(predictions)\n","\n","def crossSplitter(data, numFold):\n","\tdataset_split = list()\n","\tdataset_copy = list(data)\n","\tfold_size = int(len(data) / numFold)\n","\tfor i in range(numFold):\n","\t\tfold = list()\n","\t\twhile len(fold) < fold_size:\n","\t\t\tindex = randrange(len(dataset_copy))\n","\t\t\tfold.append(dataset_copy.pop(index))\n","\t\tdataset_split.append(fold)\n","\treturn dataset_split\n","\n","def accuracyCalc(ground, prediction):\n","\ttrueCount = 0\n","\tfor i in range(len(ground)):\n","\t\tif ground[i] == prediction[i]:\n","\t\t\ttrueCount += 1\n","\treturn  100.0 * trueCount / float(len(ground))\n","\n","def accuracyChecker(dataset, classifier, numFolds, *args):\n","\tfolds = crossSplitter(dataset, numFolds)\n","\taccuracyValues = list()\n","\tfor j in folds:\n","\t\ttrainingSet = list(folds)\n","\t\ttrainingSet.remove(j)\n","\t\ttrainingSet = sum(trainingSet, [])\n","\t\ttestingSet = list()\n","\t\tfor row in j:\n","\t\t\trow_copy = list(row)\n","\t\t\ttestingSet.append(row_copy)\n","\t\t\trow_copy[-1] = None\n","\t\tprediction = classifier(trainingSet, testingSet, *args)\n","\t\tground = [row[-1] for row in j]\n","\t\taccuracy = accuracyCalc(ground, prediction)\n","\t\taccuracyValues.append(accuracy)\n","\treturn accuracyValues\n","\n","\n","seed(5)\n","\n","os.chdir('C:/Users/senih/Desktop/Coding/VS/Python/ee485')\n","dataset = pd.read_csv(\"cardio_train.csv\", header= 0, sep= \";\")\n","\n","dataset = dataset.to_numpy()\n","\n","dataset = dataset[:1000, :] # We will focus on efficiency of the algorithm to make it faster\n","# print(np.shape(dataset))\n","\n","dataset = dataset.tolist()\n","\n","numFolds = 5\n","maxDepth = 5\n","minSize = 10\n","\n","accuracyValues = accuracyChecker(dataset, main_tree, numFolds, maxDepth, minSize)\n","print('accuracyValues: %s' % accuracyValues)\n","print('Mean Accuracy: %.3f%%' % (sum(accuracyValues)/float(len(accuracyValues))))"],"metadata":{"id":"Fvam524TdzMT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["SVM - Kernel (Not in use)"],"metadata":{"id":"Zfwp-YEi9eVW"}},{"cell_type":"code","source":["import numpy as np \n","import cvxopt\n","import cvxopt.solvers\n","\n","class SVM():\n","  def __init__(self,polyconst=1,gamma=10,degree=2):\n","    self.polyconst = float(1)\n","    self.gamma = float(gamma)\n","    self.degree = degree\n","    self._support_vectors = None\n","    self._alphas = None\n","    self.intercept = None\n","    self._n_support = None\n","    self._support_labels = None\n","    self._indices = None\n","\n","  def transform(self,X):\n","    K = np.zeros([X.shape[0],X.shape[0]])\n","    for i in range(X.shape[0]):\n","      print(i)\n","      for j in range(X.shape[0]):\n","        K[i,j] = np.exp(-1.0*self.gamma*np.dot(np.subtract(X[i],X[j]).T,np.subtract(X[i],X[j])))\n","    return K\n","\n","  def fit(self,data,labels):\n","    num_data, num_features = data.shape\n","    labels = labels.astype(np.double)\n","    alphas = np.ravel(cvxopt.solvers.qp(cvxopt.matrix(np.outer(labels,labels)*self.transform(data)),\n","                                        cvxopt.matrix(np.ones(num_data)*-1),\n","                                        cvxopt.matrix(labels,(1,num_data)),\n","                                        cvxopt.matrix(0.0),\n","                                        cvxopt.matrix(np.diag(np.ones(num_data) * -1)),\n","                                        cvxopt.matrix(np.zeros(num_data)))['x'])\n","    is_sv = alphas>1e-5\n","    self._support_vectors = data[is_sv]\n","    self._n_support = np.sum(is_sv)\n","    self._alphas = alphas[is_sv]\n","    self._support_labels = labels[is_sv]\n","    self._indices = np.arange(num_data)[is_sv]\n","    self.intercept = 0\n","    for i in range(self._alphas.shape[0]):\n","      self.intercept += self._support_labels[i] \n","      self.intercept -= np.sum(self._alphas*self._support_labels*K[self._indices[i],is_sv]) \n","    self.intercept /= self._alphas.shape[0]\n","    \n","  def signum(self,X):\n","    return np.where(X>0,1,-1)\n","\n","  def project(self,X):\n","    score = np.zeros(X.shape[0])\n","    for i in range(X.shape[0]):\n","      s = 0\n","      for alpha,label,sv in zip(self._alphas,self._support_labels,self._support_vectors):\n","        s += alpha*label*np.exp(-1.0*self.gamma*np.dot(np.subtract(X[i],sv).T,np.subtract(X[i],sv)))\n","      score[i] = s\n","    score = score + self.intercept\n","    return score\n","\n","  def predict(self,X):\n","    return self.signum(self.project(X))"],"metadata":{"id":"TskXLbEYFICp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["set1, data1 = splitter(1, data, 0.01)\n","set2, data2 = splitter(1, data1, 0.01)\n","x_train, y_train = label_design(normalization(set1))\n","x_test, y_test = label_design(normalization(set2))\n","model = SVM(gamma=3)\n","model.fit(x_train,y_train)"],"metadata":{"id":"wTNzEDKVBtxb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predictions = model.predict(x_test)"],"metadata":{"id":"nrZpvJ8oX-87"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_guess = model.predict(x_test)\n","control_param = 0\n","result = np.sum(y_guess == y_test)/np.size(y_test)\n","print(result)"],"metadata":{"id":"KViVm36xOkEW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Others (Not in use)"],"metadata":{"id":"bU2FpNrgSJZE"}},{"cell_type":"code","source":["X = 35000\n","K = np.zeros([X,X])"],"metadata":{"id":"_0ovJp8HMFSG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["KNN"],"metadata":{"id":"Yz528k7NfDWH"}},{"cell_type":"code","source":["def Euclidean_distance(x1,x2):\n","  return math.sqrt(np.sum((x1 - x2)**2))"],"metadata":{"id":"iSawHwnPWllA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def knn(x_train, y_train, test, k): # takes one test instance and returns one label\n","# we need to make this weighted.\n","  distance = []\n","  for i in range(len(x_train)-1):\n","    distance.append(Euclidean_distance(x_train[i],test))\n","  dist_arr = np.array(distance) \n","  array_inds = dist_arr.argsort()\n","  #sorted_dist = dist_arr[array_inds[::-1]]\n","  y_train_sorted = y_train[array_inds[::-1]]\n","  label_sum = np.sum(y_train_sorted[0:k-1])\n","  label = 0\n","  if label_sum/k > 0:\n","    label = 1\n","  return label"],"metadata":{"id":"-wlZs_8Tenx-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_train, y_train = label_design(normalization(train))\n","x_test, y_test = label_design(normalization(test))\n","accurate = 0\n","trained_labels = []\n","for i in range(x_test.shape[0]):\n","  trained_labels.append(knn(x_train, y_train, x_test[i], int(math.sqrt(len(y_train)))))\n","  if (trained_labels[i] == y_test[i]):\n","    accurate += 1\n","    print(i, accurate/(i+1))"],"metadata":{"id":"B_lM152Wf7Bb","colab":{"base_uri":"https://localhost:8080/","height":240},"executionInfo":{"status":"error","timestamp":1651589164896,"user_tz":-180,"elapsed":341,"user":{"displayName":"Mert Umurhan","userId":"15347790837865452233"}},"outputId":"8ec57e4b-4c2d-4cca-e769-ce7bfc230ae0"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-ead44aa17ff2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_design\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_design\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0maccurate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrained_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"]}]}]}